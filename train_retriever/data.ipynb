{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal of `data.ipynb`: generate dataset to train a dense retriever to retrieve problems given a natural language query describing a desired type of problem \n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Import OpenAI client\n",
    "\n",
    "def load_from_jsonl(file_path, show_progress = False):\n",
    "    lines = []\n",
    "    with open(file_path, encoding=\"utf8\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "    line_dicts = []\n",
    "    if show_progress:\n",
    "        for line in lines:\n",
    "            line_dicts.append(json.loads(line))\n",
    "    else:\n",
    "        line_dicts = [json.loads(line) for line in lines]\n",
    "    df = pd.DataFrame(line_dicts)\n",
    "    return df\n",
    "\n",
    "# Load OpenAI client\n",
    "client = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n"
     ]
    }
   ],
   "source": [
    "# Load and process MATH dataset: https://paperswithcode.com/dataset/math\n",
    "dataset = {}\n",
    "dataset_rows = []\n",
    "for dir in [\"train\", \"test\"]:\n",
    "    for category in os.listdir(os.path.join(\"MATH\", dir)):\n",
    "        category_path = os.path.join(\"MATH\", dir, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "        for filename in os.listdir(category_path):\n",
    "            with open(os.path.join(\"MATH\", dir, category, filename)) as f:\n",
    "                entry = json.load(f)\n",
    "                id = f\"{dir}_{category}_{filename[:-len('.json')]}\"\n",
    "                entry[\"id\"] = id\n",
    "                dataset[id] = entry\n",
    "                dataset_rows.append(entry)\n",
    "with open(\"MATH_dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f)\n",
    "with open(\"MATH_dataset_rows.json\", \"w\") as f:\n",
    "    json.dump(dataset_rows, f)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load MATH dataset\n",
    "df = pd.read_json(\"MATH_dataset_rows.json\")\n",
    "id_to_index = {}\n",
    "for index, row in df.iterrows():\n",
    "    id_to_index[row[\"id\"]] = index\n",
    "# Load few-shot examples\n",
    "few_shot_df = pd.read_csv(\"few_shot.csv\")\n",
    "train_ids = []\n",
    "# Load train IDs\n",
    "with open(\"MATH_train_ids.json\") as f:\n",
    "    train_ids = json.load(f)\n",
    "train_indices = [id_to_index[id] for id in train_ids]\n",
    "train_df = df.iloc[train_indices]\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Select training indices\n",
    "df = pd.read_json(\"MATH_dataset_rows.json\")\n",
    "train_indices = random.choices(range(len(df)), k = 1000)\n",
    "train_ids = [df.iloc[i][\"id\"] for i in train_indices]\n",
    "with open(\"MATH_train_ids.json\", \"w\") as f:\n",
    "    json.dump(train_ids, f)\n",
    "print(len(train_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "problem     What is the remainder when 369,963 is divided ...\n",
       "level                                                 Level 1\n",
       "type                                            Number Theory\n",
       "solution    If a number is divisible by 6, it must be divi...\n",
       "id                                    train_number_theory_421\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot prompt for generating a synthetic query for a given problem\n",
    "prompts = {}\n",
    "for index, row in df.iterrows():\n",
    "    few_shot = few_shot_df.iloc[random.choices(range(len(few_shot_df)), k = 3)]\n",
    "    few_shot_queries = [few_shot.iloc[i][\"query\"] for i in range(3)]\n",
    "    few_shot_problems = [df.iloc[id_to_index[few_shot.iloc[i][\"answer_id\"]]][\"problem\"] for i in range(3)]\n",
    "    prompts[row[\"id\"]] = f\"\"\"You are given a math problem. Your task is to generate a query for Problem #4 from the perspective of someone who is trying to search for math problems similiar to Problem #4. You must use the exact same writing style as the example questions #1, #2, and #3 provided below. You will be penalized if your query is longer than 10 words.\n",
    "\n",
    "Problem #1: {few_shot_problems[0]}\n",
    "Query #1: {few_shot_queries[0]}\n",
    "\n",
    "Problem #2: {few_shot_problems[1]}\n",
    "Query #2: {few_shot_queries[1]}\n",
    "\n",
    "Problem #3: {few_shot_problems[2]}\n",
    "Query #3: {few_shot_queries[2]}\n",
    "\n",
    "Problem #4: {row['problem']}\n",
    "Query #4: \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a math problem. Your task is to generate a query for Problem #4 from the perspective of someone who is trying to search for math problems similiar to Problem #4. You must use the exact same writing style as the example questions #1, #2, and #3 provided below. You will be penalized if your query is longer than 10 words.\n",
      "\n",
      "Problem #1: There are 20 people in my club.  8 of them are left-handed.  15 of them like jazz music.  2 of them are right-handed and dislike jazz music.  How many club members are left-handed and like jazz music? Assume people are either left-handed or right-handed, but not both.\n",
      "Query #1: I want a mutual exclusivity question\n",
      "\n",
      "Problem #2: A basketball player made the following number of free throws in 8 successive games: 6, 18, 15, 14, 19, 12, 19, and 15. What is the median number of successful free throws?\n",
      "Query #2: I would like a question about mean, median, or mode\n",
      "\n",
      "Problem #3: What is the area, in square units, of a regular hexagon inscribed in a circle whose area is $324\\pi$ square units? Express your answer in simplest radical form.\n",
      "Query #3: I want to get better at geometry\n",
      "\n",
      "Problem #4: What is the remainder when 369,963 is divided by 6?\n",
      "Query #4: \n"
     ]
    }
   ],
   "source": [
    "print(prompts[\"train_number_theory_421\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_to_index = {}\n",
    "count = 0\n",
    "for index, row in train_df.iterrows():\n",
    "    count += 1\n",
    "    train_id_to_index[row[\"id\"]] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = set()\n",
    "for filename in os.listdir(\"generated_queries\"):\n",
    "    done.add(filename[:-len(\".txt\")])\n",
    "print(len(done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "regen = [\"test_algebra_2822\", \"test_intermediate_algebra_1723\", \"train_counting_and_probability_573\", \"train_number_theory_510\"]\n",
    "for index, row in train_df.iterrows():\n",
    "    count += 1\n",
    "    if row[\"id\"] in done:\n",
    "        continue\n",
    "    if not row[\"id\"] in regen:\n",
    "        continue\n",
    "    with open(os.path.join(\"generated_queries\", f\"{row['id']}.txt\"), \"w\") as f:\n",
    "        response = client.chat_completions_create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompts[row[\"id\"]]}\n",
    "            ],\n",
    "            model=\"gpt-4o-mini\",\n",
    "            n=1\n",
    "        )\n",
    "        response_text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        f.write(response_text)\n",
    "        print(count, row[\"id\"], response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950\n"
     ]
    }
   ],
   "source": [
    "train_queries = {}\n",
    "for filename in os.listdir(\"generated_queries\")[:950]:\n",
    "    with open(os.path.join(\"generated_queries\", filename)) as f:\n",
    "        train_queries[filename[:-len(\".txt\")]] = f.read()\n",
    "print(len(train_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = set()\n",
    "with open(\"train.jsonl\", \"w\") as f: \n",
    "    for id in train_queries:\n",
    "        entry = {}\n",
    "        entry[\"query_id\"] = f\"{id}_query\"\n",
    "        entry[\"query\"] = train_queries[id]\n",
    "        entry[\"positive_passages\"] = [{\n",
    "            \"docid\": id,\n",
    "            \"title\": df.iloc[id_to_index[id]][\"problem\"][:50],\n",
    "            \"text\": df.iloc[id_to_index[id]][\"problem\"]\n",
    "        }]\n",
    "        random_id = df.iloc[random.choice(range(len(df)))][\"id\"]\n",
    "        while random_id == id:\n",
    "            random_id = df.iloc[random.choice(range(len(df)))][\"id\"]\n",
    "        entry[\"negative_passages\"] = [{\n",
    "            \"docid\": random_id,\n",
    "            \"title\": df.iloc[id_to_index[random_id]][\"problem\"][:50],\n",
    "            \"text\": df.iloc[id_to_index[random_id]][\"problem\"]\n",
    "        }]\n",
    "        train_queries[id]\n",
    "        f.write(json.dumps(entry))\n",
    "        f.write(\"\\n\")\n",
    "    for index, row in few_shot_df.iterrows():\n",
    "        entry = {}\n",
    "        entry[\"query_id\"] = f\"{row['answer_id']}_query\"\n",
    "        id = row[\"answer_id\"]\n",
    "        if id in ids:\n",
    "            print(\"Duplicate id\", id)\n",
    "        else:\n",
    "            ids.add(id)\n",
    "        entry[\"query\"] = row[\"query\"]\n",
    "        entry[\"positive_passages\"] = [{\n",
    "            \"docid\": row[\"answer_id\"],\n",
    "            \"title\": df.iloc[id_to_index[id]][\"problem\"][:50],\n",
    "            \"text\": df.iloc[id_to_index[id]][\"problem\"]\n",
    "        }]\n",
    "        random_id = df.iloc[random.choice(range(len(df)))][\"id\"]\n",
    "        while random_id == id:\n",
    "            random_id = df.iloc[random.choice(range(len(df)))][\"id\"]\n",
    "        entry[\"negative_passages\"] = [{\n",
    "            \"docid\": random_id,\n",
    "            \"title\": df.iloc[id_to_index[random_id]][\"problem\"][:50],\n",
    "            \"text\": df.iloc[id_to_index[random_id]][\"problem\"]\n",
    "        }]\n",
    "        f.write(json.dumps(entry))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store train data as parquet file\n",
    "train_data_df = load_from_jsonl(\"train.jsonl\")\n",
    "train_data_df.sample(frac = 1)\n",
    "train_data_df.to_parquet(f\"train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store problem corpus as parquet file\n",
    "with open(\"corpus_math.jsonl\", \"w\") as f:\n",
    "    for index, row in df.iterrows():\n",
    "        entry = {}\n",
    "        entry[\"docid\"] = row[\"id\"]\n",
    "        entry[\"title\"] = row[\"problem\"][:50]\n",
    "        entry[\"text\"] = row[\"problem\"]\n",
    "        f.write(json.dumps(entry))\n",
    "        f.write(\"\\n\")\n",
    "corpus_df = load_from_jsonl(\"corpus_math.jsonl\")\n",
    "corpus_df.to_parquet(f\"corpus_math.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 768)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "corpus_path = \"corpus_math.pkl\"\n",
    "with open(corpus_path, \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "print(embeddings[0].shape)\n",
    "\n",
    "# Create CSV file with structured data and embeddings for IRIS Vector Search\n",
    "import csv\n",
    "with open(\"embeddings.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"category\", \"difficulty\", \"embedding\"])\n",
    "    for index, row in df.iterrows():\n",
    "        id = row[\"id\"]\n",
    "        category = row[\"type\"]\n",
    "        difficulty = row[\"level\"]\n",
    "        embedding = embeddings[0][index]\n",
    "        writer.writerow([id, category, difficulty, embedding])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
